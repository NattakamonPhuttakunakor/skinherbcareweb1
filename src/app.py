from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import os
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords
from pythainlp import normalize
from flasgger import Swagger
import re

app = Flask(__name__)
CORS(app)
Swagger(app)

# -----------------------------
# Enhanced Stopwords
# -----------------------------
CUSTOM_STOPWORDS = set(thai_stopwords()) | {
    "‡πÄ‡∏õ‡πá‡∏ô", "‡∏°‡∏µ", "‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£", "‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏°‡∏≤‡∏Å", "‡πÜ", "‡∏Ñ‡πà‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö",
    "‡∏Ñ‡∏∑‡∏≠", "‡∏ó‡∏µ‡πà", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡∏ä‡πà‡∏ß‡∏¢", "‡∏î‡πâ‡∏ß‡∏¢", "‡πÅ‡∏•‡πâ‡∏ß", "‡∏à‡∏∞", "‡πÉ‡∏´‡πâ", "‡πÑ‡∏î‡πâ",
    "‡πÑ‡∏õ", "‡∏°‡∏≤", "‡∏ô‡∏∞", "‡∏Ñ‡πà‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö", "‡πÄ‡∏´‡∏£‡∏≠", "‡∏≠‡πà‡∏∞", "‡∏≠‡∏∞", "‡∏Æ‡∏∞"
}

# ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (Boosting Keywords)
SYMPTOM_KEYWORDS = {
    "‡∏™‡∏¥‡∏ß": 2.5, "‡πÅ‡∏ú‡∏•‡∏™‡∏¥‡∏ß": 2.5, "‡∏£‡∏≠‡∏¢‡∏™‡∏¥‡∏ß": 2.0, "‡∏™‡∏¥‡∏ß‡∏≠‡∏±‡∏Å‡πÄ‡∏™‡∏ö": 3.0,
    "‡∏Ñ‡∏±‡∏ô": 2.5, "‡∏Ñ‡∏±‡∏ô‡∏°‡∏≤‡∏Å": 3.0, "‡∏Ñ‡∏±‡∏ô‡πÜ": 2.0,
    "‡πÅ‡∏î‡∏á": 2.0, "‡∏≠‡∏±‡∏Å‡πÄ‡∏™‡∏ö": 2.5, "‡∏ö‡∏ß‡∏°": 2.5, "‡πÅ‡∏ú‡∏•": 2.5,
    "‡∏ú‡∏∑‡πà‡∏ô": 2.5, "‡∏ú‡∏∑‡πà‡∏ô‡∏Ñ‡∏±‡∏ô": 3.0, "‡∏ú‡∏∑‡πà‡∏ô‡πÅ‡∏î‡∏á": 2.5,
    "‡∏´‡∏ô‡∏≠‡∏á": 2.5, "‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏≠‡∏á": 3.0, "‡∏°‡∏µ‡∏´‡∏ô‡∏≠‡∏á": 2.5,
    "‡πÅ‡∏´‡πâ‡∏á": 2.0, "‡∏•‡∏≠‡∏Å": 2.5, "‡∏•‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏∏‡∏¢": 2.5,
    "‡∏î‡∏≥": 1.8, "‡∏à‡∏∏‡∏î": 1.5, "‡∏£‡∏≠‡∏¢": 1.5, "‡∏´‡∏±‡∏ß‡∏î‡∏≥": 2.0,
    "‡πÄ‡∏´‡∏á‡∏∑‡πà‡∏≠": 1.8, "‡∏°‡∏±‡∏ô": 1.8, "‡∏ú‡∏¥‡∏ß‡∏°‡∏±‡∏ô": 2.0
}

LOCATION_KEYWORDS = {
    "‡∏´‡∏ô‡πâ‡∏≤": 1.5, "‡∏´‡∏ô‡πâ‡∏≤‡∏ú‡∏≤‡∏Å": 2.0, "‡πÅ‡∏Å‡πâ‡∏°": 2.0, "‡∏Ñ‡∏≤‡∏á": 2.0, "‡∏à‡∏°‡∏π‡∏Å": 1.8,
    "‡∏´‡∏•‡∏±‡∏á": 1.8, "‡∏≠‡∏Å": 1.8, "‡πÅ‡∏Ç‡∏ô": 1.5, "‡∏Ç‡∏≤": 1.5, "‡∏°‡∏∑‡∏≠": 1.5, "‡πÄ‡∏ó‡πâ‡∏≤": 1.5,
    "‡∏´‡∏π‡∏î‡πâ‡∏≤‡∏ô‡πÉ‡∏ô": 2.0, "‡∏´‡∏±‡∏ß": 1.8, "‡∏Ñ‡∏≠": 1.5
}

def normalize_text(text):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞ normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    if not text or pd.isna(text):
        return ""
    
    text = str(text)
    # Normalize Thai text (‡∏£‡∏ß‡∏°‡∏™‡∏∞‡∏Å‡∏î‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î)
    text = normalize.normalize(text)
    # ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÄ‡∏ß‡πâ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
    text = re.sub(r'[^\u0E00-\u0E7Fa-zA-Z0-9\s]', ' ', text)
    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def enhanced_thai_tokenizer(text):
    """Tokenizer ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏° Keyword Boosting"""
    text = normalize_text(text)
    words = word_tokenize(text, engine="newmm")
    
    result = []
    for w in words:
        w = w.strip()
        # ‡∏Å‡∏£‡∏≠‡∏á stopwords ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        if w and w not in CUSTOM_STOPWORDS and len(w) > 1:
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Boosting)
            if w in SYMPTOM_KEYWORDS:
                boost_count = int(SYMPTOM_KEYWORDS[w])
                result.extend([w] * boost_count)
            elif w in LOCATION_KEYWORDS:
                boost_count = int(LOCATION_KEYWORDS[w])
                result.extend([w] * boost_count)
            else:
                result.append(w)
    
    return result

# -----------------------------
# Load Data
# -----------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data.xlsx")

if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå data.xlsx ‡∏ó‡∏µ‡πà {DATA_PATH}")

data = pd.read_excel(DATA_PATH)
data.columns = data.columns.str.strip()

def safe(val):
    return "" if pd.isna(val) else str(val)

def build_knowledge(row):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Knowledge Base ‡πÇ‡∏î‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"""
    parts = []
    
    # ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å (‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î - ‡∏ã‡πâ‡∏≥ 4 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
    main_symptom = safe(row.get('‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å'))
    parts.extend([main_symptom] * 4)
    
    # ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏á (‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏•‡∏≤‡∏á - ‡∏ã‡πâ‡∏≥ 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
    secondary = safe(row.get('‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏á'))
    parts.extend([secondary] * 2)
    
    # ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢ (‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏•‡∏≤‡∏á - ‡∏ã‡πâ‡∏≥ 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
    location = safe(row.get('‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢'))
    parts.extend([location] * 3)
    
    # ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ (‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏•‡∏≤‡∏á - ‡∏ã‡πâ‡∏≥ 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
    disease = safe(row.get('‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ'))
    parts.extend([disease] * 2)
    
    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏Å‡∏©‡∏≤ (‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡πà‡∏≥ - ‡∏ã‡πâ‡∏≥ 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
    treatment = safe(row.get('‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏ï‡πâ‡∏ô'))
    parts.append(treatment)
    
    return ' '.join(parts)

data['knowledge'] = data.apply(build_knowledge, axis=1)

# -----------------------------
# Enhanced TF-IDF Model
# -----------------------------
vectorizer = TfidfVectorizer(
    tokenizer=enhanced_thai_tokenizer,
    ngram_range=(1, 3),  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô trigram
    min_df=1,  # ‡∏•‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡∏≤‡∏Å
    max_df=0.95,  # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    sublinear_tf=True,  # ‡∏•‡∏î weight ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏°‡∏≤‡∏Å
    use_idf=True,
    smooth_idf=True
)
tfidf_matrix = vectorizer.fit_transform(data['knowledge'])

# -----------------------------
# Similarity Calculation
# -----------------------------
def calculate_similarity_with_features(user_input, data_df):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡πÅ‡∏ö‡∏ö Multi-Feature"""
    
    # 1. TF-IDF Similarity (70% weight)
    user_vec = vectorizer.transform([user_input])
    tfidf_scores = cosine_similarity(user_vec, tfidf_matrix).flatten()
    
    # 2. Keyword Match Score (20% weight)
    user_tokens = set(enhanced_thai_tokenizer(user_input))
    keyword_scores = []
    
    for idx, row in data_df.iterrows():
        disease_tokens = set(enhanced_thai_tokenizer(row['knowledge']))
        # Jaccard Similarity
        intersection = len(user_tokens & disease_tokens)
        union = len(user_tokens | disease_tokens)
        keyword_score = intersection / union if union > 0 else 0
        keyword_scores.append(keyword_score)
    
    keyword_scores = np.array(keyword_scores)
    
    # 3. Location Match Bonus (10% weight)
    location_scores = np.zeros(len(data_df))
    for keyword in LOCATION_KEYWORDS.keys():
        if keyword in user_input:
            for idx, row in data_df.iterrows():
                location_field = safe(row.get('‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢', ''))
                if keyword in location_field:
                    location_scores[idx] += 0.3
    
    # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
    final_scores = (
        tfidf_scores * 0.7 + 
        keyword_scores * 0.2 + 
        location_scores * 0.1
    )
    
    return final_scores

# -----------------------------
# Routes
# -----------------------------
@app.route("/")
def health_check():
    return jsonify({
        "status": "ok",
        "service": "SkinHerbCare Symptom AI (Enhanced)",
        "version": "2.0",
        "endpoints": {
            "diagnose": "/api/analysis/diagnose",
            "stats": "/api/analysis/stats"
        }
    })

@app.route("/api/analysis/diagnose", methods=["POST"])
def diagnose():
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡∏±‡∏á - ‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    ---
    tags:
      - Diagnosis
    parameters:
      - in: body
        name: body
        required: true
        schema:
          type: object
          properties:
            symptoms:
              type: string
              description: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏°‡∏≤‡πÄ‡∏≠‡∏á
    responses:
      200:
        description: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏≠‡∏Å‡∏°‡∏≤
    """
    payload = request.get_json(silent=True) or {}
    user_input = payload.get("symptoms", "").strip()

    if not user_input:
        return jsonify({
            "success": False, 
            "message": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å"
        }), 400

    # Validate input length
    if len(user_input) < 3:
        return jsonify({
            "success": False,
            "message": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)"
        }), 400

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡πÅ‡∏ö‡∏ö Multi-Feature
    scores = calculate_similarity_with_features(user_input, data)
    
    # ‡∏´‡∏≤ Top 3 candidates
    top_indices = np.argsort(scores)[::-1][:3]
    top_scores = scores[top_indices]
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if top_scores[0] < 0.1:  # threshold
        return jsonify({
            "success": True,
            "data": {
                "disease": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ",
                "confidence": 0,
                "advice": "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ö‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡∏±‡∏á",
                "herbs": [],
                "suggestions": [
                    "‡∏•‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏ô‡πâ‡∏≤ ‡πÅ‡∏Ç‡∏ô ‡∏Ç‡∏≤",
                    "‡∏£‡∏∞‡∏ö‡∏∏‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏±‡∏ô ‡πÅ‡∏î‡∏á ‡∏ö‡∏ß‡∏° ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏≠‡∏á",
                    "‡∏ö‡∏≠‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏à‡πá‡∏ö ‡πÅ‡∏™‡∏ö ‡∏£‡πâ‡∏≠‡∏ô"
                ]
            }
        })
    
    # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏Å
    main_idx = top_indices[0]
    main_row = data.iloc[main_idx]
    main_score = top_scores[0]
    
    # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏≠‡∏á (‡∏ñ‡πâ‡∏≤ confidence ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô)
    alternatives = []
    for i in range(1, 3):
        if top_scores[i] >= main_score * 0.75:  # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô
            alt_row = data.iloc[top_indices[i]]
            alternatives.append({
                "disease": safe(alt_row.get("‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ")),
                "confidence": round(float(top_scores[i]) * 100, 2)
            })
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á response
    herbs_raw = safe(main_row.get("‡∏™‡∏°‡∏∏‡∏ô‡πÑ‡∏û‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", ""))
    herbs_list = [h.strip() for h in herbs_raw.split(",") if h.strip()] if herbs_raw else []
    
    return jsonify({
        "success": True,
        "data": {
            "disease": safe(main_row.get("‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ")),
            "confidence": round(float(main_score) * 100, 2),
            "advice": safe(main_row.get("‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏ï‡πâ‡∏ô")),
            "herbs": herbs_list,
            "symptoms_matched": safe(main_row.get("‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å")),
            "common_location": safe(main_row.get("‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢")),
            "alternative_possibilities": alternatives if alternatives else None
        }
    })

@app.route("/api/analysis/stats", methods=["GET"])
def get_stats():
    """‡∏î‡∏π‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö"""
    return jsonify({
        "total_diseases": len(data),
        "total_features": tfidf_matrix.shape[1],
        "model_info": {
            "type": "TF-IDF + Multi-Feature",
            "ngram_range": "1-3",
            "features": ["Keyword Boosting", "Location Matching", "Jaccard Similarity"]
        }
    })

# -----------------------------
# Run App
# -----------------------------
if __name__ == "__main__":
    print("\n" + "="*60)
    print("üöÄ Enhanced SkinHerbCare AI Server Starting...")
    print(f"üìä Loaded {len(data)} diseases")
    print(f"üéØ Model: TF-IDF + Keyword Boosting + Multi-Feature")
    print(f"üîç Features: {tfidf_matrix.shape[1]} unique features")
    print("="*60 + "\n")
    app.run(port=5001, debug=True)
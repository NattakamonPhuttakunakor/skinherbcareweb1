from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords
import sys
import os

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Server
app = Flask(__name__)
CORS(app) # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ Node.js ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏Ñ‡∏∏‡∏¢‡πÑ‡∏î‡πâ

# ========================================
# üîß ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Synonym (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÉ‡∏´‡πâ AI ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)
# ========================================
SYNONYM_MAP = {
    '‡∏õ‡∏ß‡∏î': ['‡πÄ‡∏à‡πá‡∏ö', '‡πÅ‡∏™‡∏ö', '‡∏ö‡∏ß‡∏°', '‡∏≠‡∏±‡∏Å‡πÄ‡∏™‡∏ö', '‡∏à‡∏∏‡∏Å‡πÅ‡∏ô‡πà‡∏ô', '‡∏ó‡∏£‡∏°‡∏≤‡∏ô', '‡∏õ‡∏ß‡∏î‡πÅ‡∏™‡∏ö', '‡∏£‡πâ‡∏≠‡∏ô'],
    '‡∏Ñ‡∏±‡∏ô': ['‡∏Ñ‡∏±‡∏ô‡πÜ', '‡∏Ñ‡∏±‡∏ô‡∏°‡∏≤‡∏Å', '‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Å‡∏≤', '‡∏¢‡∏∏‡∏ö‡∏¢‡∏¥‡∏ö', '‡∏¢‡∏¥‡∏ö‡πÜ'],
    '‡∏ú‡∏∑‡πà‡∏ô': ['‡∏ú‡∏∑‡πà‡∏ô‡πÅ‡∏î‡∏á', '‡∏ú‡∏∑‡πà‡∏ô‡∏Ñ‡∏±‡∏ô', '‡∏ï‡∏∏‡πà‡∏°', '‡∏ï‡∏∏‡πà‡∏°‡πÅ‡∏î‡∏á', '‡∏õ‡∏∑‡πâ‡∏ô', '‡∏•‡∏°‡∏û‡∏¥‡∏©', '‡∏ï‡∏∏‡πà‡∏°‡πÉ‡∏™', '‡πÄ‡∏°‡πá‡∏î'],
    '‡πÑ‡∏Ç‡πâ': ['‡∏°‡∏µ‡πÑ‡∏Ç‡πâ', '‡∏ï‡∏±‡∏ß‡∏£‡πâ‡∏≠‡∏ô', '‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏Ç‡πâ', '‡∏£‡∏∏‡∏°‡πÜ', '‡∏Ñ‡∏£‡∏±‡πà‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ñ‡∏£‡∏±‡πà‡∏ô‡∏ï‡∏±‡∏ß'],
    '‡∏õ‡∏≤‡∏Å': ['‡∏£‡∏¥‡∏°‡∏ù‡∏µ‡∏õ‡∏≤‡∏Å', '‡∏°‡∏∏‡∏°‡∏õ‡∏≤‡∏Å', '‡∏´‡∏ô‡πâ‡∏≤', '‡πÅ‡∏Å‡πâ‡∏°'],
    '‡πÅ‡∏Ç‡∏ô': ['‡∏ï‡πâ‡∏ô‡πÅ‡∏Ç‡∏ô', '‡∏õ‡∏•‡∏≤‡∏¢‡πÅ‡∏Ç‡∏ô', '‡∏Ç‡πâ‡∏≠‡∏®‡∏≠‡∏Å', '‡∏°‡∏∑‡∏≠', '‡∏ô‡∏¥‡πâ‡∏ß'],
    '‡∏Ç‡∏≤': ['‡∏ï‡πâ‡∏ô‡∏Ç‡∏≤', '‡∏ô‡πà‡∏≠‡∏á', '‡πÄ‡∏ó‡πâ‡∏≤', '‡πÄ‡∏Ç‡πà‡∏≤'],
    '‡∏°‡∏≤‡∏Å': ['‡∏°‡∏≤‡∏Å‡πÜ', '‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á', '‡πÄ‡∏¢‡∏≠‡∏∞', '‡∏´‡∏ô‡∏±‡∏Å', '‡πÑ‡∏°‡πà‡πÑ‡∏´‡∏ß'],
}

CUSTOM_STOPWORDS = set(thai_stopwords()) | {
    "‡πÄ‡∏õ‡πá‡∏ô", "‡∏°‡∏µ", "‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£", "‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏°‡∏≤‡∏Å", "‡πÜ", "‡∏Ñ‡πà‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö", 
    "‡∏Ñ‡∏∑‡∏≠", "‡∏ó‡∏µ‡πà", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡∏ä‡πà‡∏ß‡∏¢", "‡∏î‡πâ‡∏ß‡∏¢", "‡πÅ‡∏•‡πâ‡∏ß", "‡∏≠‡∏¢‡∏≤‡∏Å", "‡∏ï‡πâ‡∏≠‡∏á",
    "‡∏ô‡∏∞", "‡∏à‡∏∞", "‡πÄ‡∏≠‡∏á", "‡πÑ‡∏î‡πâ", "‡πÑ‡∏õ", "‡∏°‡∏≤", "‡∏≠‡∏¢‡∏π‡πà", "‡πÉ‡∏´‡πâ", "‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì", "‡πÅ‡∏ñ‡∏ß‡πÜ", "‡∏°‡∏±‡∏ô"
}

def expand_synonyms(text):
    text = str(text).lower()
    for main_word, synonyms in SYNONYM_MAP.items():
        for syn in synonyms:
            if syn in text:
                text += f" {main_word}" 
    return text

def thai_tokenizer(text):
    if not isinstance(text, str): return []
    text = expand_synonyms(text) 
    words = word_tokenize(text, engine="newmm", keep_whitespace=False)
    return [w for w in words if w not in CUSTOM_STOPWORDS and len(w) > 1 and not w.isnumeric()]

# ========================================
# üìÇ ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ========================================
print("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô AI...")
df = None
possible_files = ["data.xlsx - Sheet1.csv", "data.csv", "data.xlsx"]

for f in possible_files:
    if os.path.exists(f):
        try:
            if f.endswith('.csv'):
                df = pd.read_csv(f)
            else:
                df = pd.read_excel(f)
            print(f"‚úÖ ‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå: {f}")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {f} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

if df is None:
    print("‚ùå Error: ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢")
    sys.exit()

df.columns = df.columns.str.strip()

def clean_and_prepare_data(row):
    main = str(row.get('‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å', ''))
    sub = str(row.get('‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏á', ''))
    loc = str(row.get('‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢', ''))
    treatment = str(row.get('‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏ï‡πâ‡∏ô', ''))
    
    if '‡πÑ‡∏Ç‡πâ' in treatment and '‡πÑ‡∏Ç‡πâ' not in sub:
        sub += " ‡∏°‡∏µ‡πÑ‡∏Ç‡πâ"
        
    # ‡πÄ‡∏ö‡∏¥‡πâ‡∏•‡∏Ñ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (Weighting)
    knowledge_text = f"{row['‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ']} {main} {main} {sub} {loc} {loc}"
    return knowledge_text

df['knowledge'] = df.apply(lambda x: clean_and_prepare_data(x), axis=1)

vectorizer = TfidfVectorizer(
    tokenizer=thai_tokenizer,
    ngram_range=(1, 2),
    min_df=1,
    sublinear_tf=True
)

try:
    tfidf_matrix = vectorizer.fit_transform(df['knowledge'])
    print(f"‚úÖ AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô! (‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å {len(df)} ‡πÇ‡∏£‡∏Ñ)")
except Exception as e:
    print(f"‚ùå Error ‡∏ï‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏°‡∏≠‡∏á AI: {e}")
    sys.exit()

# ========================================
# üîå ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: API Endpoint
# ========================================
@app.route('/api/analyze', methods=['POST'])
def analyze():
    data = request.json
    user_input = data.get('symptoms', '').strip()
    
    print(f"üì© Input: {user_input}") # Log input

    if not user_input:
        return jsonify({"success": False, "message": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏≤‡∏Å‡∏≤‡∏£"})

    user_vec = vectorizer.transform([user_input])
    scores = cosine_similarity(user_vec, tfidf_matrix).flatten()
    
    # ‡∏´‡∏≤ Top 3
    top_indices = scores.argsort()[::-1][:3]
    
    results = []
    found_any = False

    for idx in top_indices:
        score = scores[idx]
        
        # üö©üö©üö© ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡∏•‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏≤‡∏Å 0.1 ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 0.01 üö©üö©üö©
        # ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏°‡πâ‡∏Ñ‡∏≥‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏õ‡πä‡∏∞
        if score > 0.01: 
            found_any = True
            row = df.iloc[idx]
            
            # Debug: ‡∏õ‡∏£‡∏¥‡πâ‡∏ô‡∏ó‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà
            print(f"   üëâ ‡πÄ‡∏à‡∏≠‡πÇ‡∏£‡∏Ñ: {row['‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ']} (Score: {score:.4f})")

            warning_msg = ""
            if '‡πÑ‡∏Ç‡πâ' in user_input and '‡πÑ‡∏Ç‡πâ' in str(row.get('‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏á', '')):
                warning_msg = "(‡πÇ‡∏£‡∏Ñ‡∏ô‡∏µ‡πâ‡∏°‡∏±‡∏Å‡∏°‡∏µ‡πÑ‡∏Ç‡πâ‡∏£‡πà‡∏ß‡∏°‡∏î‡πâ‡∏ß‡∏¢ ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)"

            results.append({
                "disease": str(row['‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ']),
                "confidence": round(score * 100, 2),
                "symptoms": str(row['‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å']),
                "location": str(row['‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢']),
                "treatment": str(row.get('‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏ï‡πâ‡∏ô', '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå')),
                "warning": warning_msg,
                "herbs": str(row.get('‡∏™‡∏°‡∏∏‡∏ô‡πÑ‡∏û‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á', '-')).split(',')
            })

    if not found_any:
        print("   ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏û‡∏≠")
        return jsonify({
            "success": True,
            "found": False,
            "message": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        })

    return jsonify({
        "success": True,
        "found": True,
        "data": results
    })

if __name__ == '__main__':
    print("üöÄ Python Server ‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡πà Port 5001...")
    app.run(port=5001, debug=True)
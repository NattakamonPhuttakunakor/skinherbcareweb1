from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords
import sys
import os

app = Flask(__name__)
CORS(app) 

# ========================================
# üîß ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå (Synonyms)
# ========================================
# ‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏™‡πà‡πÄ‡∏¢‡∏≠‡∏∞ AI ‡∏¢‡∏¥‡πà‡∏á‡∏â‡∏•‡∏≤‡∏î ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡∏ä‡∏≤‡∏ß‡∏ö‡πâ‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
SYNONYM_MAP = {
    '‡∏õ‡∏ß‡∏î': ['‡πÄ‡∏à‡πá‡∏ö', '‡∏ó‡∏£‡∏°‡∏≤‡∏ô', '‡∏£‡∏∞‡∏ö‡∏°', '‡∏õ‡∏ß‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏¢', '‡∏ï‡∏∂‡∏á'],
    '‡πÅ‡∏™‡∏ö': ['‡∏õ‡∏ß‡∏î‡πÅ‡∏™‡∏ö‡∏õ‡∏ß‡∏î‡∏£‡πâ‡∏≠‡∏ô', '‡∏£‡πâ‡∏≠‡∏ô', '‡πÑ‡∏´‡∏°‡πâ', '‡∏£‡∏∞‡∏Ñ‡∏≤‡∏¢‡πÄ‡∏Ñ‡∏∑‡∏≠‡∏á', '‡πÅ‡∏™‡∏ö‡πÜ'],
    '‡∏Ñ‡∏±‡∏ô': ['‡∏Ñ‡∏±‡∏ô‡πÜ', '‡∏¢‡∏∏‡∏ö‡∏¢‡∏¥‡∏ö', '‡∏¢‡∏¥‡∏ö‡πÜ', '‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Å‡∏≤', '‡πÄ‡∏Å‡∏≤'],
    '‡∏ï‡∏∏‡πà‡∏°': ['‡πÄ‡∏°‡πá‡∏î', '‡∏ú‡∏∑‡πà‡∏ô', '‡∏™‡∏¥‡∏ß', '‡πÅ‡∏ú‡∏•', '‡∏£‡∏≠‡∏¢‡πÅ‡∏î‡∏á', '‡∏õ‡∏∑‡πâ‡∏ô', '‡∏à‡∏∏‡∏î‡πÅ‡∏î‡∏á'],
    '‡∏õ‡∏≤‡∏Å': ['‡∏£‡∏¥‡∏°‡∏ù‡∏µ‡∏õ‡∏≤‡∏Å', '‡∏°‡∏∏‡∏°‡∏õ‡∏≤‡∏Å', '‡∏£‡∏≠‡∏ö‡∏õ‡∏≤‡∏Å', '‡∏´‡∏ô‡πâ‡∏≤'],
    '‡πÑ‡∏Ç‡πâ': ['‡∏ï‡∏±‡∏ß‡∏£‡πâ‡∏≠‡∏ô', '‡∏£‡∏∏‡∏°‡πÜ', '‡∏Ñ‡∏£‡∏±‡πà‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ñ‡∏£‡∏±‡πà‡∏ô‡∏ï‡∏±‡∏ß', '‡∏°‡∏µ‡πÑ‡∏Ç‡πâ', '‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏Ç‡πâ'],
    '‡∏ú‡∏¥‡∏ß': ['‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡∏±‡∏á', '‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤', '‡∏´‡∏ô‡πâ‡∏≤'],
}

CUSTOM_STOPWORDS = set(thai_stopwords()) | {
    "‡πÄ‡∏õ‡πá‡∏ô", "‡∏°‡∏µ", "‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£", "‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏°‡∏≤‡∏Å", "‡πÜ", "‡∏Ñ‡πà‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö", 
    "‡∏Ñ‡∏∑‡∏≠", "‡∏ó‡∏µ‡πà", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡∏ä‡πà‡∏ß‡∏¢", "‡∏î‡πâ‡∏ß‡∏¢", "‡πÅ‡∏•‡πâ‡∏ß", "‡∏≠‡∏¢‡∏≤‡∏Å", "‡∏ï‡πâ‡∏≠‡∏á",
    "‡∏ô‡∏∞", "‡∏à‡∏∞", "‡πÄ‡∏≠‡∏á", "‡πÑ‡∏î‡πâ", "‡πÑ‡∏õ", "‡∏°‡∏≤", "‡∏≠‡∏¢‡∏π‡πà", "‡πÉ‡∏´‡πâ", "‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì", "‡πÅ‡∏ñ‡∏ß‡πÜ", "‡∏°‡∏±‡∏ô"
}

def expand_synonyms(text):
    text = str(text).lower()
    for main_word, synonyms in SYNONYM_MAP.items():
        for syn in synonyms:
            if syn in text:
                text += f" {main_word}" 
    return text

def thai_tokenizer(text):
    if not isinstance(text, str): return []
    text = expand_synonyms(text) 
    words = word_tokenize(text, engine="newmm", keep_whitespace=False)
    return [w for w in words if w not in CUSTOM_STOPWORDS and len(w) > 1 and not w.isnumeric()]

# ========================================
# üìÇ ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Excel/CSV)
# ========================================
print("‚è≥ AI: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
df = None
possible_files = ["data.xlsx - Sheet1.csv", "data.csv", "data.xlsx"]

for f in possible_files:
    if os.path.exists(f):
        try:
            if f.endswith('.csv'):
                df = pd.read_csv(f)
            else:
                df = pd.read_excel(f)
            print(f"‚úÖ AI: ‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå {f}")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è AI: ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {f} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ({e})")

if df is None:
    print("‚ùå Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (data.xlsx ‡∏´‡∏£‡∏∑‡∏≠ .csv)")
    sys.exit()

df.columns = df.columns.str.strip()

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô (‡∏£‡∏ß‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô)
def clean_and_prepare_data(row):
    main = str(row.get('‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å', ''))
    sub = str(row.get('‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏á', ''))
    loc = str(row.get('‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢', ''))
    
    # ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: ‡πÄ‡∏ö‡∏¥‡πâ‡∏•‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    knowledge_text = f"{row['‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ']} {main} {main} {sub} {loc}"
    return knowledge_text

df['knowledge'] = df.apply(lambda x: clean_and_prepare_data(x), axis=1)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏°‡∏≠‡∏á AI
vectorizer = TfidfVectorizer(
    tokenizer=thai_tokenizer,
    ngram_range=(1, 2),
    min_df=1,
    sublinear_tf=True
)

try:
    tfidf_matrix = vectorizer.fit_transform(df['knowledge'])
    print(f"‚úÖ AI: ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô! (‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å {len(df)} ‡πÇ‡∏£‡∏Ñ)")
except Exception as e:
    print(f"‚ùå Error: {e}")
    sys.exit()

# ========================================
# üîå ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: API Endpoint
# ========================================
@app.route('/api/analyze', methods=['POST'])
def analyze():
    data = request.json
    user_input = data.get('symptoms', '').strip()
    
    print(f"\nüì© ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: '{user_input}'")

    if not user_input:
        return jsonify({"success": False, "message": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏≤‡∏Å‡∏≤‡∏£"})

    user_vec = vectorizer.transform([user_input])
    scores = cosine_similarity(user_vec, tfidf_matrix).flatten()
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
    top_indices = scores.argsort()[::-1][:3]
    
    results = []
    found_any = False

    for idx in top_indices:
        score = scores[idx]
        
        # üî• ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏•‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏•‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 0.01 (1%) 
        # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏´‡∏°‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ô‡πâ‡∏≠‡∏¢
        if score > 0.01: 
            found_any = True
            row = df.iloc[idx]
            
            print(f"   üëâ ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö: {row['‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ']} (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {score:.4f})")

            warning_msg = ""
            if '‡πÑ‡∏Ç‡πâ' in user_input and '‡πÑ‡∏Ç‡πâ' in str(row.get('‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏á', '')):
                warning_msg = "(‡πÇ‡∏£‡∏Ñ‡∏ô‡∏µ‡πâ‡∏°‡∏±‡∏Å‡∏°‡∏µ‡πÑ‡∏Ç‡πâ‡∏£‡πà‡∏ß‡∏°‡∏î‡πâ‡∏ß‡∏¢ ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)"

            results.append({
                "disease": str(row['‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏Ñ']),
                "confidence": round(score * 100, 2),
                "symptoms": str(row['‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å']),
                "location": str(row['‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢']),
                "treatment": str(row.get('‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏ï‡πâ‡∏ô', '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå')),
                "warning": warning_msg,
                "herbs": str(row.get('‡∏™‡∏°‡∏∏‡∏ô‡πÑ‡∏û‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á', '-')).split(',')
            })

    if not found_any:
        print("   ‚ùå ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)")
        return jsonify({
            "success": True,
            "found": False,
            "message": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        })

    return jsonify({
        "success": True,
        "found": True,
        "data": results
    })

if __name__ == '__main__':
    print("üöÄ Python Server ‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡πà Port 5001...")
    app.run(port=5001, debug=True)